{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "drive-into-tokenizer-part-2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/menon92/DL-Sneak-Peek/blob/master/drive_into_tokenizer_part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8o6kP6ibx5k",
        "colab_type": "code",
        "colab": {},
        "outputId": "5af7119e-5ab7-4f60-8410-1eb897543641"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vkI0G9Abx5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# টেক্সট করপাস ডিফাইন করি । আমাদের এই করপাসে দুইটা বাক্য আছে \n",
        "texts = [\n",
        "    \"আমি বাংলায় কথা কই\",\n",
        "    \"আমি বাংলার কথা কই\"\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rFBI0BDbx6T",
        "colab_type": "code",
        "colab": {},
        "outputId": "755b74d3-9b6e-4a02-b575-6d80c3712a59"
      },
      "source": [
        "# ডিফল্ট সেটআপ\n",
        "tokenizer1 = tf.keras.preprocessing.text.Tokenizer(filters=',')\n",
        "\n",
        "# এই বার আমরা টোকেনাইজার ডিফাইন করার সময় বলে দিচ্ছি `num_wors=3`\n",
        "# এর ফলে `texts` এ যতগুলোই ওয়ার্ড থাকুক না কেন সে ২ টা \n",
        "# (সেটা সেট করে দিব টার থেকে ১ টা কম দিবে) ওয়ার্ড রিটার্ন করবে \n",
        "tokenizer2 = tf.keras.preprocessing.text.Tokenizer(filters=',', num_words=3)\n",
        "\n",
        "tokenizer1.fit_on_texts(texts)\n",
        "tokenizer2.fit_on_texts(texts)\n",
        "\n",
        "print(\"Word to index map of `tokenizer1`:\", tokenizer1.word_index)\n",
        "print(\"Word to index map of `tokenizer2`:\", tokenizer2.word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word to index map of `tokenizer1`: {'আমি': 1, 'কথা': 2, 'কই': 3, 'বাংলায়': 4, 'বাংলার': 5}\n",
            "Word to index map of `tokenizer2`: {'আমি': 1, 'কথা': 2, 'কই': 3, 'বাংলায়': 4, 'বাংলার': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ytYuewhbx6i",
        "colab_type": "text"
      },
      "source": [
        "আমরা দেখে পাচ্ছি দুইটা টোকেনাইজার এর জন্যই `word_index` একই । তার মানে কি আমাদের `num_words=3` কাজ করে নি ? এটা আসলে কাজ করেছে । আমরা যদি `texts_to_sequences` মেথড দেখি তাহলেই বুঝতে পারব । "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX7b-q6lbx6l",
        "colab_type": "code",
        "colab": {},
        "outputId": "f89add3c-38a6-4f49-f7bb-813b98bb4f3a"
      },
      "source": [
        "# টেক্সট কে সংখ্যায় কনভার্ট করি । টেক্সট কে এইভাবে সংখ্যায় প্রকাশ করার করা কে সিকুয়েন্স বলে \n",
        "sequence1 = tokenizer1.texts_to_sequences(texts)\n",
        "sequence2 = tokenizer2.texts_to_sequences(texts)\n",
        "\n",
        "print('Using 1st tokenizer:', sequence1)\n",
        "print('Using 2nd tokenizer:', sequence2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using 1st tokenizer: [[1, 4, 2, 3], [1, 5, 2, 3]]\n",
            "Using 2nd tokenizer: [[1, 2], [1, 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWFK7OESbx6x",
        "colab_type": "text"
      },
      "source": [
        "আমরা দেখতে পাচ্ছি যে প্রথম টোকেনাইজার সবগুলো নিয়েছে কিন্তু ২য় টোকেনাইজার শুধু ২ টা শব্দ নিয়েছে । এখন প্রশ্ন থাকতে পারে `word_index` জন্য কাজ `num_words=3` করল না কেন ? এটা কাজ করে নি কারণ যখন `fit_on_texts` কল করা হয় তখন টোকেনাইজার `word_index, index_word, word_counts, word_docs` এই ভেরিয়েবলগুলকে আপডেট করবে পুরা টেক্সট ডাটার উপর । \n",
        "\n",
        "`num_words=3` এর প্রভাব পরবে `sequences_to_texts, texts_to_sequences, sequences_to_matrix, texts_to_matrix` এই ধরনের মেথড গুলতে । \n",
        "\n",
        "এখন আমরা বুঝতে পারলাম আমরা চাইলে আমাদের ইচ্ছা মত ওয়ার্ড (`num_words`) সেট করে দিয়ে লোড করতে পারি ।\n",
        "\n",
        "####  কিন্তু টোকেনাইজার কিভবে `num_words` এর বেশি ওয়ার্ড কে বাদ দেয় ? \n",
        "আমরা আগের মত একটা উদাহরণ টা আবার দেখি "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXl86D_3bx61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts_small = [\n",
        "    \"আমি বাংলায় কথা কই\",\n",
        "    \"আমি বাংলার কথা কই\"]\n",
        "\n",
        "texts_large = [\n",
        "    \"আমি বাংলায় কথা কই\",\n",
        "    \"আমি বাংলার কথা কই\",\n",
        "    \"আমি বাংলায় ভাসি, বাংলায় হাসি, বাংলায় জেগে রই\",\n",
        "    \"আমি বাংলায় ভাসি, বাংলায় হাসি, বাংলায় জেগে রই\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMkFdJXPbx7C",
        "colab_type": "code",
        "colab": {},
        "outputId": "a96087a1-4c2b-4e2f-f2e3-3f1a4bb6bffa"
      },
      "source": [
        "tokenizer_small = tf.keras.preprocessing.text.Tokenizer(filters=',')\n",
        "tokenizer_large = tf.keras.preprocessing.text.Tokenizer(filters=',')\n",
        "\n",
        "tokenizer_small.fit_on_texts(texts_small)\n",
        "tokenizer_large.fit_on_texts(texts_large)\n",
        "\n",
        "print(\"Tokenizer small:\", tokenizer_small.word_index)\n",
        "print(\"Tokenizer lerge:\", tokenizer_large.word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer small: {'আমি': 1, 'কথা': 2, 'কই': 3, 'বাংলায়': 4, 'বাংলার': 5}\n",
            "Tokenizer lerge: {'বাংলায়': 1, 'আমি': 2, 'কথা': 3, 'কই': 4, 'ভাসি': 5, 'হাসি': 6, 'জেগে': 7, 'রই': 8, 'বাংলার': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPknoSZnbx7O",
        "colab_type": "text"
      },
      "source": [
        "উপরে আমরা দেখতে পাচ্ছি `tokenizer_small` এ `আমি` শব্দ প্রথমে চলে আসছে । `tokenizer_large` এ দেখতে পাচ্ছি `বাংলায়` কথা প্রথমে আসছে এটা এই রকম কেন হওয়ার পেছনে কারণ কি ? টোকেনাইজার যেটা করে সেটা হল পুরা ডাটাসেট এ যে শব্দের frequency বেশি সটা সবার প্রথমে রাখবে ২য় বেশি frequency র শব্দ ২য় পজিশনে রাখবে এবং যেটা frequency সবচেয়ে কম সেটা সেটা সবার শেষে রাখবে । আমরা `word_counts` দেখলেই বুঝতে পারব । "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1B1pdyPbx7R",
        "colab_type": "code",
        "colab": {},
        "outputId": "c8f0ae88-8fc5-4734-8009-8ff1b7a99b38"
      },
      "source": [
        "print(\"Tokenizer small:\", tokenizer_small.word_counts)\n",
        "print(\"Tokenizer large:\", tokenizer_large.word_counts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer small: OrderedDict([('আমি', 2), ('বাংলায়', 1), ('কথা', 2), ('কই', 2), ('বাংলার', 1)])\n",
            "Tokenizer large: OrderedDict([('আমি', 4), ('বাংলায়', 7), ('কথা', 2), ('কই', 2), ('বাংলার', 1), ('ভাসি', 2), ('হাসি', 2), ('জেগে', 2), ('রই', 2)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95PbCmdIbx7a",
        "colab_type": "text"
      },
      "source": [
        "যখন আমরা `Tokenizer(filters=',', num_words=3)` করি তখন উপরের frequency ধরে টপ ২ টা নিবে । "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN3-VZq9bx7c",
        "colab_type": "text"
      },
      "source": [
        "### OOV টকেন কি ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyWX0nJ8bx7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_texts = [\"আমি বাংলায় কথা কই\"]\n",
        "test_texts = [\"আমি বাংলায় কথা কই\", \"আমি বাংলার কথা কই\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcTCosKXbx7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=',')\n",
        "tokenizer.fit_on_texts(train_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7V1DPv8bx7z",
        "colab_type": "code",
        "colab": {},
        "outputId": "8b62b209-ca24-4fe3-d963-473ef6bb0b61"
      },
      "source": [
        "# \"আমি বাংলার কথা কই\" এর জন্য ৪ টা সংখ্যা না এসে ৩ আসবে কারণ \n",
        "# `বাংলার` টোকেন আমাদের `tokenizer` আগে দেখে নি । \n",
        "# যে সকল টোকেন টোকেনাইজার আগে দেখেনি সেই সকল টকেন কে বলে OOV (Out Of Vocabulary)\n",
        "tokenizer.texts_to_sequences(test_texts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4], [1, 3, 4]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDixAZfubx7-",
        "colab_type": "text"
      },
      "source": [
        "আপনি NLP তে যে মডেলই বানান না কেন, ট্রেনিং করার সময় যে টোকেনাইজার ব্যাবহার করবেন, মডেল টেস্ট করার সময় একই টোকেনাইজার ব্যবহার করতে হবে । এর ফলে এটা হওয়ার সম্ভবনা খুব বেশি যে আপনার টেস্ট ডটাতে oov কিছু টোকেন থাকবে । এই সমস্যা হ্যান্ডলে করার জন্য আমাদের টোকেনাইজার এ নতুন একটা প্যারামিটার এড করব । "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9TRgfu9bx8A",
        "colab_type": "code",
        "colab": {},
        "outputId": "1120ff0b-199f-4cc2-a9ee-0402868fdd5a"
      },
      "source": [
        "# oov_token=\"<OOV>\" দিয়ে আমরা বলে দিলাম যদি এমন কোন টোকেন/শব্দ আশে \n",
        "# সেটা আমাদের টোকেনাইজার দেখে নি তাহলে সে সেই টোকেন কে <OOV> দিয়ে রিপ্লেস করবে \n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=',', oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# oov আগের সমস্যা শলভ\n",
        "tokenizer.texts_to_sequences(test_texts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 3, 4, 5], [2, 1, 4, 5]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVVO7cQ3bx8L",
        "colab_type": "code",
        "colab": {},
        "outputId": "190cd41e-0ac4-41ad-fd30-02ac7b943774"
      },
      "source": [
        "# আমরা word_index এ দেখতে পাচ্ছি '<OOV>': 1 সেট করেছে \n",
        "tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<OOV>': 1, 'আমি': 2, 'বাংলায়': 3, 'কথা': 4, 'কই': 5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfq8930Fbx8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}