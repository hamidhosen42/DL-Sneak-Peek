{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# গ্রাডিয়েন্ট-টেপ \n",
    "\n",
    "**এই নোটবুকে আমরা দেখব গ্রাডিয়েন্ট টেপ কি এবং এটার বিভিন্ন ব্যবহার । এবং এটা কিভাবে ব্যবহার করা যায় ।**\n",
    "\n",
    "আমরা একটু ইন্টারমিডিয়েটে ফিরে যাই । "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y = x^2 + 2x + 5` সমীকরণের `(x, y)=(2, 5)` বিন্দুতে `ঢাল / গ্র্যাডিয়েন্ট / স্লোপ` কত ? \n",
    "\n",
    "এটার সমাধান করতে হলে \n",
    "\n",
    "**প্রথমত:** সমীকরণকে x এর সাপেক্ষে ডেরিভেটিভ করতে হবে । অর্থাৎ \n",
    "\n",
    "`d/dx(y) = 2x + 2 ------ (১)`\n",
    "\n",
    "**দ্বিতীয়ত:** (x, y) = (2, 5) বিন্দু থেকে x এর মান সমীকরণ (১) এ বসাতে হবে । \n",
    "\n",
    "`x = 2`\n",
    "\n",
    "`2*2 + 2 = 6`\n",
    "\n",
    "সুতরাং `(2, 5)` দিন্দুতে ঢাল `6`\n",
    "\n",
    "এই ধরনের সমস্যার সমাধান আমি ইন্টারমিডিয়েটে অনেক করেছি । কিন্তু এটা অ্যাপ্লিকেশন যে কি হবে পারে তখন সেই সময় কোন ধারনাই ছিল না । বর্তমান সময়ের ট্রেন্ড টেকনোলজিতে (মেশিন লার্নিং, ডিপ লার্নিং) এটার বিশাল বর অ্যাপ্লিকেশন আছে । ডিপ লার্নিং এলগরিদম গুলোর প্রধান কাজ হল অপটিমাইজ ওয়েট খুঁজে বের করা করা। সঠিক ওয়েট খুঁজে না পেলে এলগরিদম কাজই করবে না । মজার ব্যাপার হল এই ওয়েট খুঁজে বের করার জন্য সেই ইন্টারমিডিয়েটে করা সেই গণিতটাই কাজে লাগতেছে ।   \n",
    "\n",
    "এখন গ্রাডিয়েন্ট-টেপে ফিরে আশা যাক । আমরা উপরে যে সমস্যার সমাধান করলাম সেই কাজটা আমরা যদি টেন্সরফ্লো তে করতে চাই তাহলে আমরা গ্রাডিয়েন্ট-টেপ ব্যবহার করে খুব সহজেই করতে পারি । এটা পাইথনের [contex manager](https://book.pythontips.com/en/latest/context_managers.html) এর মত করে ব্যবহার করা যায় । এটা ব্যবহার করে যেকোনো ধরনের সমীকরণের গ্র্যাডিয়েন্ট হিসাব করা যায় । তবে শর্ত হল সমীকরণকে অবশ্যই ডিফারেনসিয়েবল হবে হবে । \n",
    "\n",
    "একটা ফাংশন ডিফারেনসিয়েবল কি না সেটা পরীক্ষা কিভাবে করা হয় সেটা জানতে চাইলে দেখতে পারেন [ডিফারেনসিয়েবিলিটি পরীক্ষা](https://www.mathsisfun.com/calculus/differentiable.html)\n",
    "\n",
    "গ্র্যাডিয়েন্ট কে রিকল করতে এইটা দেখতে পারেন [গ্র্যাডিয়েন্ট](https://www.mathsisfun.com/gradient.html)\n",
    "\n",
    "উপরের সমীকরণ থেকে আমরা যদি `tf.GradientTape` দিয়ে গ্র্যাডিয়েন্ট বের করতে চাই তাহলে এই ভাবে করা যেতে পারে । "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant(2.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x**2 + 2*x + 5\n",
    "\n",
    "# `x` এর সাপেক্ষে `y` এর গ্র্যাডিয়েন্ট কত ?\n",
    "gradient = tape.gradient(y, x).numpy()\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "দেখা যাচ্ছে যে গ্র্যাডিয়েন্ট ৬ এসেছে । এবং এটাই আশার কথা । এখানে একটা বিষয় লক্ষণীয় যে আমরা `tape.watch(x)` ব্যবহার করেছি । এখন আপনার টেন্সর যদি ভেরিয়েবল টাইপ হয় তাহলে তাহলে `tape.watch(x)` ব্যবহার করার দরকার নাই । বাই-ডিফল্ট সে ওয়াচ করবে । কিন্তু আপনার টেন্সর যদি constant টাইপ হয় তাহলে আপনাকে `tape.watch(x)` আলাদা ভাবে লিখতে হবে ।  না লিখলে `tape.gradient(y, x)` NoneType টাইপ রিটান করবে । আপনি `tape.watch(x)` কমেন্ট আউট করে দেখতে পারেন ।  \n",
    "\n",
    "এখন আমরা `tf.Variable` দিয়ে একটা উদাহরণ দেখি । এখানে আমাদের `tape.gradient(y, x)` ব্যবহার করার প্রয়োজন হয় নি ।  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2 + 2*x + 5\n",
    "    gred = tape.gradient(y, x)\n",
    "    print(gred.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher-Order Derivatives কিভাবে করতে হবে \n",
    "\n",
    "আপনি যদি চাইলে একাধিক ভেরিয়েবলের জন্য গ্র্যাডিয়েন্ট হিসাব করতে পারবেন । সেটা করার জন্য আপনাকে লিস্ট আকারে ভেরিয়েবল পাঠাতে হবে । মনে করেন আপনি `x, y` দুইটার জন্যই গ্র্যাডিয়েন্ট হিসাব করতে চান । এটা করার জন্য আপনাকে এই ভাবে করতে হবে ।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient is: 6.0\n",
      "Gradient is: 12.0\n"
     ]
    }
   ],
   "source": [
    "# (x, y) = (2.0, 5.0)\n",
    "# for x = 2, 2x + 2 = 2*2 + 2 = 6\n",
    "# for y = 5, 2y + 2 = 5*2 + 2 = 12\n",
    "\n",
    "x = tf.Variable(2.0)\n",
    "y = tf.Variable(5.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    eq_x = x**2 + 2*x + 5\n",
    "    eq_y = y**2 + 2*y + 5\n",
    "    greds = tape.gradient([eq_x, eq_y], [x, y])\n",
    "    for gred in greds:\n",
    "        print('Gradient is:', gred.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এই একাধিক ভেরিয়েবলের জন্য গ্র্যাডিয়েন্ট হিসাব করাটা খুব কাজে দেয় । আমার যখন `keras` মডেল ট্রেইন করি তখন `model.trainable_variables` এর মাধ্যমে ভেরিয়েবলের লিস্ট `tf.GradientTape()` এর কাছে পাঠিয়ে দেই । "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "আমাদের যদি হাইয়ার অর্ডার ডেরিভেটিভ করতে হয় তাহলে আমরা সেটা একাধিক `tf.GradientTape()` ব্যবহার করে করতে পারি । মনে করেন আমাদের এই রকম একটা ইকুয়েশন আছে । \n",
    "\n",
    "`y = x^3 + 2x + 1` এখন আমরা এটাকে যদি দুইবার ডেরিভেটিভ করি তাহলে,\n",
    "\n",
    "`y1 = 3x^2 + 2`\n",
    "\n",
    "`y2 = 2*3x`\n",
    "\n",
    "এখন x = 2 বসালে আমরা পাই\n",
    "\n",
    "`y1 = 3*(2)^2 + 2 = 14`\n",
    "\n",
    "`y2 = 2*3*2 = 12`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for 1st order derivative: 14.0\n",
      "Gradient for 2nd order derivative: 12.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as tape:\n",
    "        eq = x**3 + 2*x + 1\n",
    "        y1 = tape.gradient(eq, x)\n",
    "        print('Gradient for 1st order derivative:', y1.numpy())\n",
    "    y2 = outer_tape.gradient(y1, x)\n",
    "    print('Gradient for 2nd order derivative:', y2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# গ্রাডিয়েন্ট-টেপের কিছু বিশেষ ব্যবহার \n",
    "\n",
    "###   1 - watch_accessed_variables=False\n",
    "\n",
    "আমরা যদি চাই যখন কোন একটা ভেরিয়েবল কে ওয়াচ করব না তাহলে আমাদের কে `watch_accessed_variables=False` ফ্ল্যাগ ব্যবহার করতে হবে । "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    y = x**3\n",
    "\n",
    "print(tape.gradient(y, x)) # -> None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**এখন কথা হল কোন একটা ভেরিয়েবল কে ওয়াচ না করার অর্থ কি ?** \n",
    "\n",
    "ওয়াচ না করলে সে ওই ভেরিয়েবলের জন্য গ্র্যাডিয়েন্ট হিসাব করবে না । যার ফলে মডেল ট্রেইনিং করার সময় ওয়েট আপডেট হবে না । \n",
    "\n",
    "**এখন কথা হল এটা কেন দরকার ?**\n",
    "\n",
    "ডিপ লার্নিং এ **ট্রান্সফার লার্নিং** নামে একটা টেকনিক আছে । এখানে একটা অলরেডি র্লান মডেলের জ্ঞান নতুন মডেলে ট্রান্সফার করার হয় । মনে করেন আপনর অলরেডি র্লান মডেলে ১০০ টা অপটিমাইজেবল ভেরিয়েবল আছে । এখানে যেটা করা হয় নতুন মডেল ট্রেইনিং করার সময় এই ১০০ টা অপটিমাইজেবল ভেরিয়েবল অপটিমাইজ না করে (ওয়েট উপডেট হবে না) কিছু ভেরিয়েবল কে অপটিমাইজ করা ।  \n",
    "\n",
    "এখনে আমরা যে ভেরিয়বল গুলোর ওয়েট আপডেট করতে চাইনা সেগুলোর ক্ষেত্রে `watch_accessed_variables=False` করে দিব । তাহলে মডেল ট্রেইনিং করার সময় ওয়েট আপডেট হবে না । "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - persistent=True\n",
    "\n",
    "আমরা যদি নিচের কোড রান করি তাহলে তাহলে অউতপুট হওয়ার কথা \n",
    "\n",
    "`12.0`\n",
    "\n",
    "`12.0`\n",
    "\n",
    "কিন্তু এই রকম প্রিন্ট হবে না । ২য় প্রিন্ট এ গিয়ে আপনি `RuntimeError: GradientTape.gradient can only be called once on non-persistent tapes.` ইরর খাবেন । আমরা যে tepe ওপেন করেছি সেটা একটা `non-persistent tapes` আর এই ধরনের ক্ষেত্রে টেপ কেবল মাত্র একবারই কল করা যায় । এই ধরনের টেপের ক্ষেত্রে একবার গ্র্যাডিয়েন্ট হিসাব করার জন্য যত ধরনের ইনফরমেশন স্টোর করা হয়েছিল সবগুলো ডিলিট করে দেয় । যার কারণ পরের বার যখন সে গ্র্যাডিয়েন্ট হিসাব করতে যায় তখন সেই ইনফরমেশন গুলো খুঁজে পায় না । "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(6.0, trainable=True)\n",
    "b = tf.Variable(2.0, trainable=True)\n",
    "with tf.GradientTape() as tape:\n",
    "    y1 = a ** 2\n",
    "    y2 = b ** 3\n",
    "                                                                                                                                                                                                                                                                                                                                                \n",
    "print(tape.gradient(y1, a).numpy())\n",
    "print(tape.gradient(y2, b).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এখন আমরা যদি `tf.GradientTape()` এর এই বৈশিষ্ট্য ওভাররাইট করতে চাই তাহলে আমাদের কে `persistent=True` করতে হবে নিচের মত করে । এখন কোন সমস্যা ছাড়াই কোড রান হবে । "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(6.0)\n",
    "b = tf.Variable(2.0)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y1 = a ** 2\n",
    "    y2 = b ** 3\n",
    "                                                                                                                                                                                                                                                                                                                                                \n",
    "print(tape.gradient(y1, a).numpy())\n",
    "print(tape.gradient(y2, b).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এটা সম্পর্কে আরও জানতে, \n",
    "\n",
    "- [GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape)\n",
    "- [gradienttape-explained-for-keras-users](https://medium.com/analytics-vidhya/tf-gradienttape-explained-for-keras-users-cc3f06276f22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2.x] *",
   "language": "python",
   "name": "conda-env-tf2.x-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
